# ğŸš€ Projeto Data Lake & Pipeline de ETL

![Python](https://img.shields.io/badge/python-3.10+-blue)
![PySpark](https://img.shields.io/badge/pyspark-3.4.0-orange)
![Delta Lake](https://img.shields.io/badge/delta-lake-green)
![Poetry](https://img.shields.io/badge/poetry-dependencies-blueviolet)
![Docker](https://img.shields.io/badge/docker-ready-informational)
![License](https://img.shields.io/badge/license-MIT-lightgrey)

Este projeto implementa um **pipeline de dados** com arquitetura em camadas (**Bronze**, **Silver** e **Gold**) usando **PySpark**, **Delta Lake** e **AWS S3**.  
O objetivo Ã© transformar dados brutos em **insights prontos para anÃ¡lise de negÃ³cio**, garantindo **qualidade, governanÃ§a e escalabilidade**.

---

## ğŸ— Arquitetura do Pipeline

A estrutura do projeto segue o seguinte fluxo:

* **API** â†’ **Raw/Landingzone** â†’ **Bronze** â†’ **Silver** â†’ **Gold**

O pipeline segue a **arquitetura em 3 camadas**:

| Camada  | FunÃ§Ã£o                                                                 |
|---------|------------------------------------------------------------------------|
| **Landing Zone / Raw** | Armazena os dados brutos coletados da API. MantÃ©m histÃ³rico de arquivos originais. |
| **Bronze** | Dados ingeridos e armazenados em Delta Lake. Preserva dados originais e garante rastreabilidade. |
| **Silver** | Dados limpos, validados e enriquecidos: tipagem, normalizaÃ§Ã£o e deduplicaÃ§Ã£o. |
| **Gold**   | Dados agregados e sumarizados, prontos para consumo por ferramentas de BI. |

---

## âš™ï¸ Estrutura do Projeto

```

.
â”œâ”€â”€ data
â”‚   â””â”€â”€ raw
â”‚       â””â”€â”€ salarios_*.parquet
â”œâ”€â”€ docs
â”‚   â””â”€â”€ Pipeline ETL API.html
â”œâ”€â”€ log
â”‚   â””â”€â”€ log.txt
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ dashboard
â”‚   â”‚   â””â”€â”€ Painel_ETL.py
â”‚   â””â”€â”€ python
â”‚       â”œâ”€â”€ 01_ingestao_api.py
â”‚       â”œâ”€â”€ 02_envio_landingzone_aws.py
â”‚       â”œâ”€â”€ 03_carga_bronze_aws.py
â”‚       â”œâ”€â”€ 04_transform_silver_aws.py
â”‚       â””â”€â”€ 05_gera_gold_aws.py
â”œâ”€â”€ .env
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ poetry.lock
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â””â”€â”€ run_pipeline.sh

```

- `data/raw/` â†’ Armazena os arquivos brutos baixados da API.  
- `log/` â†’ Armazena logs do pipeline (`log.txt`).  
- `src/python/` â†’ Scripts de ETL (ordem sequencial).  
- `src/dashboard/` â†’ Painel interativo em Streamlit (`Painel_ETL.py`).  
- `run_pipeline.sh` â†’ Script para execuÃ§Ã£o sequencial com Docker.  

---

## ğŸ”§ ConfiguraÃ§Ã£o

Crie um arquivo `.env` na raiz do projeto (ou copie de `.env.example`) e configure as variÃ¡veis de ambiente:

```env
# DefiniÃ§Ãµes de ambiente
LOG_DIR=./log
LOG_FILE=log.txt
RAW_DIR=./data/raw
RAW_FILE=dados_brutos.parquet

# InformaÃ§Ãµes para API
LINK_API=https://api-dados-abertos.cearatransparente.ce.gov.br/transparencia/servidores/salarios
ANO_API=2025
MES_API=6
PAGINA_INI=1

# Destinos folders AWS S3
LANDINGZONE_DIR=landingzone
BRONZE_DIR=bronze
SILVER_DIR=silver
GOLD_DIR=gold

# Credenciais AWS
AWS_ACCESS_KEY_ID=CHAVEACESSOAWS
AWS_SECRET_ACCESS_KEY=CHAVESECRETAAWS
AWS_REGION=REGAOAWS
S3_BUCKET_NAME=BUCKETAWS
```

> **Dica:** Sempre use `.env` para nÃ£o expor credenciais sensÃ­veis.

---

## ğŸ“¦ InstalaÃ§Ã£o

Instale as dependÃªncias com Poetry:

```bash
poetry install --no-root
```

---

## ğŸƒâ€â™‚ï¸ Formas de ExecuÃ§Ã£o

Atualmente o projeto possui **3 modos de execuÃ§Ã£o**:

### ğŸ”¹ 1. ExecuÃ§Ã£o direta individual

Executando cada script manualmente via terminal:

```bash
poetry run python src/python/01_ingestao_api.py
poetry run python src/python/02_envio_landingzone_aws.py
poetry run python src/python/03_carga_bronze_aws.py
poetry run python src/python/04_transform_silver_aws.py
poetry run python src/python/05_gera_gold_aws.py
```

---

### ğŸ”¹ 2. ExecuÃ§Ã£o sequencial via `run_pipeline.sh`

Arquivo que executa todos os scripts na ordem correta, dentro de um container Docker:

```bash
./run_pipeline.sh
```

ConteÃºdo do arquivo:

```bash
#!/bin/bash
set -e

docker run --rm -it --env-file .env pipeline-etl poetry run python src/python/01_ingestao_api.py
docker run --rm -it --env-file .env pipeline-etl poetry run python src/python/02_envio_landingzone_aws.py
docker run --rm -it --env-file .env pipeline-etl poetry run python src/python/03_carga_bronze_aws.py
docker run --rm -it --env-file .env pipeline-etl poetry run python src/python/04_transform_silver_aws.py
docker run --rm -it --env-file .env pipeline-etl poetry run python src/python/05_gera_gold_aws.py
```

---

### ğŸ”¹ 3. ExecuÃ§Ã£o via Docker + Dashboard Streamlit

A imagem jÃ¡ estÃ¡ publicada no Docker Hub:
ğŸ‘‰ [felipealvss/pipeline-etl](https://hub.docker.com/r/felipealvss/pipeline-etl)

Basta executar:

```bash
docker pull felipealvss/pipeline-etl
docker run -p 8501:8501 --env-file .env felipealvss/pipeline-etl
```

Isso iniciarÃ¡ um **painel Streamlit** no navegador, onde cada botÃ£o executa individualmente os scripts do pipeline de forma **interativa e visual**.

---

## ğŸ“ˆ Futuras Melhorias

* Suporte a mÃºltiplas fontes de dados
* OmplementaÃ§Ãµes de testes automatizados (com Pytest)
* Monitoramento e alertas automÃ¡ticos do pipeline
* Versionamento e histÃ³rico dos datasets Delta

---

## ğŸ“š ReferÃªncias

* [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
* [Delta Lake Documentation](https://delta.io/)
* [AWS S3 Documentation](https://aws.amazon.com/s3/)
* [Poetry - Python Dependency Management](https://python-poetry.org/)
* [Streamlit](https://streamlit.io/)
* [Docker Hub - pipeline-etl](https://hub.docker.com/r/felipealvss/pipeline-etl)

---

## ğŸ’¡ Dicas

* Explore os scripts em `src/python/` para customizar transformaÃ§Ãµes.
* Use o painel `Painel_ETL.py` para uma experiÃªncia mais interativa.
* Monitore logs em `log/log.txt` para depuraÃ§Ã£o e auditoria.
* Ajuste os caminhos no `.env` conforme seu ambiente local ou bucket S3.
