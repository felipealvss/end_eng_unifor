# üöÄ Projeto Data Lake & Pipeline de ETL

![Python](https://img.shields.io/badge/python-3.10+-blue)
![PySpark](https://img.shields.io/badge/pyspark-3.4.0-orange)
![Delta Lake](https://img.shields.io/badge/delta-lake-green)
![Poetry](https://img.shields.io/badge/poetry-dependencies-blueviolet)
![License](https://img.shields.io/badge/license-MIT-lightgrey)

Este projeto implementa um **pipeline de dados** com arquitetura em camadas (**Bronze**, **Silver** e **Gold**) usando **PySpark**, **Delta Lake** e **AWS S3**.  
O objetivo √© transformar dados brutos em **insights prontos para an√°lise de neg√≥cio**, garantindo **qualidade, governan√ßa e escalabilidade**.

---

## üèó Arquitetura do Pipeline

O pipeline segue a **arquitetura em 3 camadas**:

| Camada  | Fun√ß√£o                                                                 |
|---------|------------------------------------------------------------------------|
| **Landing Zone / Raw** | Armazena os dados brutos coletados da API. Mant√©m hist√≥rico de arquivos originais. |
| **Bronze** | Dados ingeridos e armazenados em Delta Lake. Preserva dados originais e garante rastreabilidade. |
| **Silver** | Dados limpos, validados e enriquecidos: tipagem, normaliza√ß√£o e deduplica√ß√£o. |
| **Gold**   | Dados agregados e sumarizados, prontos para consumo por ferramentas de BI. |

---

## ‚öôÔ∏è Estrutura do Projeto

```

.
‚îú‚îÄ‚îÄ data
‚îÇ   ‚îî‚îÄ‚îÄ raw
‚îÇ       ‚îî‚îÄ‚îÄ salarios_*.parquet
‚îú‚îÄ‚îÄ log
‚îÇ   ‚îî‚îÄ‚îÄ log.txt
‚îú‚îÄ‚îÄ poetry.lock
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ src
‚îî‚îÄ‚îÄ python
‚îú‚îÄ‚îÄ 01_ingestao_api.py
‚îú‚îÄ‚îÄ 02_envio_landingzone_aws.py
‚îú‚îÄ‚îÄ 03_carga_bronze_aws.py
‚îú‚îÄ‚îÄ 04_transform_silver_aws.py
‚îî‚îÄ‚îÄ 05_gera_gold_aws.py

```

- `data/raw/` ‚Üí Armazena os arquivos brutos baixados da API.  
- `log/` ‚Üí Armazena logs do pipeline (`log.txt`).  
- `src/python/` ‚Üí Cont√©m os scripts de ETL, seguindo a ordem de execu√ß√£o.

---

## üîß Configura√ß√£o

Crie um arquivo `.env` na raiz do projeto (ou copie de `.env.example`) e configure as vari√°veis de ambiente:

```env
# Defini√ß√µes de ambiente
LOG_DIR = ./log
LOG_FILE = log.txt
RAW_DIR = ./data/raw
RAW_FILE = dados_brutos.parquet

# Informa√ß√µes para API
LINK_API = "https://api-dados-abertos.cearatransparente.ce.gov.br/transparencia/servidores/salarios"
ANO_API = 2025
MES_API = 6
PAGINA_INI = 1

# Destinos folders AWS S3
LANDINGZONE_DIR = landingzone
BRONZE_DIR = bronze
SILVER_DIR = silver
GOLD_DIR = gold

# Credenciais AWS
AWS_ACCESS_KEY_ID=CHAVEACESSOAWS
AWS_SECRET_ACCESS_KEY=CHAVESECRETAAWS
AWS_REGION=REGAOAWS
S3_BUCKET_NAME=BUCKETAWS
```

> **Dica:** Sempre use `.env` para n√£o expor credenciais sens√≠veis.

---

## üì¶ Instala√ß√£o

Instale as depend√™ncias com Poetry:

```bash
poetry install -no-root
```

---

## üèÉ‚Äç‚ôÇÔ∏è Execu√ß√£o do Pipeline

O pipeline √© executado na **ordem dos scripts**, garantindo que os dados fluam corretamente do raw at√© o Gold:

### 1Ô∏è‚É£ Ingest√£o da API

Coleta os dados da API e salva localmente:

```bash
poetry run python src/python/01_ingestao_api.py
```

### 2Ô∏è‚É£ Envio para Landing Zone AWS

Envia os arquivos brutos para o bucket S3:

```bash
poetry run python src/python/02_envio_landingzone_aws.py
```

### 3Ô∏è‚É£ Carga Bronze

Ingest√£o dos dados na camada Bronze em Delta Lake:

```bash
poetry run python src/python/03_carga_bronze_aws.py
```

### 4Ô∏è‚É£ Transforma√ß√£o Silver

Aplica limpeza, normaliza√ß√£o e deduplica√ß√£o:

```bash
poetry run python src/python/04_transform_silver_aws.py
```

### 5Ô∏è‚É£ Gera√ß√£o Gold

Executa agrega√ß√µes e gera datasets prontos para an√°lise:

```bash
poetry run python src/python/05_gera_gold_aws.py
```

---

## üìà Futuras Melhorias

* Suporte a m√∫ltiplas fontes de dados
* Integra√ß√£o com ferramentas de BI (PowerBI, Tableau)
* Monitoramento e alertas autom√°ticos do pipeline
* Versionamento e hist√≥rico dos datasets Delta

---

## üìö Refer√™ncias

* [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
* [Delta Lake Documentation](https://delta.io/)
* [AWS S3 Documentation](https://aws.amazon.com/s3/)
* [Poetry - Python Dependency Management](https://python-poetry.org/)

---

## üí° Dicas

* Explore os scripts em `src/python/` para customizar transforma√ß√µes.
* Monitore logs em `log/log.txt` para depura√ß√£o e auditoria.
* Ajuste os caminhos no `.env` conforme seu ambiente local ou bucket S3.
